{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.1.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jenniferzhuang/Library/Python/3.9/lib/python/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jenniferzhuang/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jenniferzhuang/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jenniferzhuang/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training utterances and answers\n",
    "with open(\"data/train_utt.txt\", \"r\") as f:\n",
    "    utterances = f.readlines()\n",
    "\n",
    "with open(\"./data/train_ans.txt\", \"r\") as f:\n",
    "    slots = f.readlines()\n",
    "\n",
    "# Strip whitespace\n",
    "utterances = [utt.strip() for utt in utterances]\n",
    "slots = [slot.strip() for slot in slots]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing slot annotations into dictionaries and pairing them with their respective utterances. This part was intended to help us understanding the parsing process for our utterances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future implementation: for higher-level intent, we should do use_cases and do single canonical use cases. For example \"school\", \"work\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance 1: Can you show me laptops from a specific brand, like Apple?\n",
      "{'brand': 'Apple'}\n",
      "\n",
      "Utterance 2: I need a budget-friendly laptop. What’s the cheapest one you have?\n",
      "{'price': 'cheap'}\n",
      "\n",
      "Utterance 3: I need a laptop with a powerful processor for heavy tasks. Any suggestions?\n",
      "{'processor_tier': 'high'}\n",
      "\n",
      "Utterance 4: Can you recommend a laptop with at least a Core i5 processor?\n",
      "{'processor_tier': 'i5+'}\n",
      "\n",
      "Utterance 5: I need a compact laptop with a smaller display. What are my options?\n",
      "{'display_size': 'small'}\n",
      "\n",
      "Utterance 6: What’s the most affordable laptop with a 15.6-inch display?\n",
      "{'display_size': '15.6', 'price': 'cheap'}\n",
      "\n",
      "Utterance 7: I am trying to find a laptop that has a core i9 processor and display size of 15.6, what are my options?\n",
      "{'processor_tier': 'i9', 'display_size': '15.6'}\n",
      "\n",
      "Utterance 8: I am looking specifically for a Samsung laptop.\n",
      "{'brand': 'Samsung'}\n",
      "\n",
      "Utterance 9: I want a laptop that has at least 16GB RAM and doesn’t cost more than $500. Any options?\n",
      "{'ram_memory': '16GB', 'price': '500'}\n",
      "\n",
      "Utterance 10: What’s the best laptop for coding with lots of RAM and a decent processor?\n",
      "{'ram_memory': '16GB+', 'processor_tier': 'decent'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_slots(slot):\n",
    "    pairs = slot.split(\"|\")[1:] \n",
    "    slot_dict = {}\n",
    "    for pair in pairs:\n",
    "        key, value = pair.split(\"=\")\n",
    "        slot_dict[key] = value\n",
    "    return slot_dict\n",
    "\n",
    "# Convert slot strings to dictionaries\n",
    "slot_dicts = [parse_slots(slot) for slot in slots]\n",
    "\n",
    "# Combine utterances with their parsed slot dictionaries\n",
    "utterance_slot_pairs = list(zip(utterances, slot_dicts))\n",
    "\n",
    "# Display the parsed dictionaries\n",
    "for i, (utterance, slot_dict) in enumerate(utterance_slot_pairs[:10]):  # Limit to first 10\n",
    "    print(f\"Utterance {i+1}: {utterance}\")\n",
    "    print(f\"{slot_dict}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find_laptop': defaultdict(set, {})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "intent_dic = {'find_laptop':defaultdict(set)}\n",
    "intent_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'find_laptop': defaultdict(<class 'set'>, {'brand': {'Dell', 'lenovo', 'ASUS', 'Microsoft', 'HP', 'Apple', 'Samsung'}, 'price': {'cheap', '700', 'expensive', 'mid-range', '500'}, 'processor_tier': {'decent', 'high', 'm1', 'i7', 'i5+', 'i9', 'core i7', 'm2', 'latest'}, 'display_size': {'15', '14', '15.6', 'small', '13'}, 'ram_memory': {'16GB', '256GB+', '16GB+', 'max', '8GB', '8GB+', '32GB'}, 'brand!': {'Apple'}})}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize dictionary for 'find_laptop' intent\n",
    "intent_dic = {'find_laptop': defaultdict(set)}\n",
    "\n",
    "# Placeholder for context and answers related to laptops\n",
    "laptop_context = []\n",
    "laptop_answers = []\n",
    "\n",
    "# File paths to the dataset\n",
    "utt_file_path = \"data/train_utt.txt\"\n",
    "ans_file_path = \"data/train_ans.txt\"\n",
    "\n",
    "# Process dataset to extract laptop-related information\n",
    "with open(utt_file_path, 'r') as f1:\n",
    "    with open(ans_file_path, 'r') as f2:\n",
    "        utterances = f1.readlines()\n",
    "        for i, line in enumerate(f2.readlines()):\n",
    "            a_line = line.strip()\n",
    "            ans = a_line.split('|')  \n",
    "            intent = ans[0]  # Extract the intent\n",
    "            if intent == 'find_laptop':  # Only process 'find_laptop' intent\n",
    "                for a in ans[1:]:\n",
    "                    if '=' in a:\n",
    "                        slot, value = a.split('=')  # Split slot-value pair\n",
    "                        laptop_context.append(utterances[i].strip())  # Save context\n",
    "                        laptop_answers.append(value)  # Save the value\n",
    "                        intent_dic[intent][slot].add(value)  # Add slot-value pair to intent_dic\n",
    "\n",
    "# Output the updated intent dictionary\n",
    "print(intent_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you show me laptops from a specific brand, like Apple?',\n",
       " 'I need a budget-friendly laptop. What’s the cheapest one you have?',\n",
       " 'I need a laptop with a powerful processor for heavy tasks. Any suggestions?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_context[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'cheap', 'high']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_answers[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge Case: Negation for specific "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_answer_span_tensor identifies the span of a given answer within a tokenized context when paired with a question. used in question-answering tasks.\n",
    "\n",
    "span based approach to find the start and end positions of the answer within the combined tokenized sequence of question and context. will help with superised training for BERT to allow the model to pinpoint the slot values directly in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_span_tensor(question, context, answer):\n",
    "    # Print inputs before tokenization\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "    # Tokenize inputs\n",
    "    ques = tokenizer.tokenize(question)\n",
    "    context = tokenizer.tokenize(context)\n",
    "    answer = tokenizer.tokenize(answer)\n",
    "\n",
    "    # Print tokenized inputs\n",
    "    print(\"Tokenized Question:\", ques)\n",
    "    print(\"Tokenized Context:\", context)\n",
    "    print(\"Tokenized Answer:\", answer)\n",
    "\n",
    "    # Combine tokens with special tokens\n",
    "    s = [\"[CLS]\"] + ques + [\"[SEP]\"] + context + [\"[SEP]\"]\n",
    "    print(\"Combined Tokens (with special tokens):\", s)\n",
    "\n",
    "    # Initialize start and end\n",
    "    start, end = 0, 0\n",
    "\n",
    "    # Match tokens to find the answer span\n",
    "    for id_a, token_a in enumerate(answer):\n",
    "        for id_s, token_s in enumerate(s):\n",
    "            if token_a == token_s:\n",
    "                if answer == s[id_s:id_s + len(answer)]:\n",
    "                    start = id_s\n",
    "                    end = id_s + len(answer) - 1\n",
    "                    break\n",
    "                elif len(s) - id_s + 1 <= len(answer):\n",
    "                    break\n",
    "\n",
    "    # Print start and end positions\n",
    "    print(\"Start:\", start)\n",
    "    print(\"End:\", end)\n",
    "\n",
    "    return torch.tensor([start, end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you suggest a laptop with a Core i7 processor and 16GB of RAM?\n",
      "Context: \n",
      "Available laptops:\n",
      "1. Dell Inspiron: Core i5, 8GB RAM, $800.\n",
      "2. HP Spectre x360: Core i7, 16GB RAM, $1500.\n",
      "3. Apple MacBook Pro: M1 chip, 16GB RAM, $2000.\n",
      "4. Lenovo ThinkPad: AMD Ryzen 5, 8GB RAM, $1000.\n",
      "\n",
      "Answer: HP Spectre x360\n",
      "Tokenized Question: ['can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?']\n",
      "Tokenized Context: ['available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.']\n",
      "Tokenized Answer: ['hp', 'spec', '##tre', 'x', '##36', '##0']\n",
      "Combined Tokens (with special tokens): ['[CLS]', 'can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?', '[SEP]', 'available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.', '[SEP]']\n",
      "Start: 43\n",
      "End: 48\n",
      "Answer Span Tensor: tensor([43, 48])\n"
     ]
    }
   ],
   "source": [
    "# Example inputs\n",
    "test_question = \"Can you suggest a laptop with a Core i7 processor and 16GB of RAM?\"\n",
    "test_context = \"\"\"\n",
    "Available laptops:\n",
    "1. Dell Inspiron: Core i5, 8GB RAM, $800.\n",
    "2. HP Spectre x360: Core i7, 16GB RAM, $1500.\n",
    "3. Apple MacBook Pro: M1 chip, 16GB RAM, $2000.\n",
    "4. Lenovo ThinkPad: AMD Ryzen 5, 8GB RAM, $1000.\n",
    "\"\"\"\n",
    "test_answer = \"HP Spectre x360\"\n",
    "bad_answer = \"Dell Inspiron\"  \n",
    "\n",
    "# Call function\n",
    "tensor = get_answer_span_tensor(test_question, test_context, test_answer)\n",
    "print(\"Answer Span Tensor:\", tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ## prefix in tokenized words (e.g., ##tre in the tokenized word \"spectre\") is a characteristic of subword tokenization used in models like BERT. Helps handle rare words or words that were not part of the model's training data. Instead of ignoring or failing to recognize unknown words, BERT splits them into subword tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you suggest a laptop with a Core i7 processor and 16GB of RAM?\n",
      "Context: \n",
      "Available laptops:\n",
      "1. Dell Inspiron: Core i5, 8GB RAM, $800.\n",
      "2. HP Spectre x360: Core i7, 16GB RAM, $1500.\n",
      "3. Apple MacBook Pro: M1 chip, 16GB RAM, $2000.\n",
      "4. Lenovo ThinkPad: AMD Ryzen 5, 8GB RAM, $1000.\n",
      "\n",
      "Answer: HP Spectre x360\n",
      "Tokenized Question: ['can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?']\n",
      "Tokenized Context: ['available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.']\n",
      "Tokenized Answer: ['hp', 'spec', '##tre', 'x', '##36', '##0']\n",
      "Combined Tokens (with special tokens): ['[CLS]', 'can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?', '[SEP]', 'available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.', '[SEP]']\n",
      "Start: 43\n",
      "End: 48\n",
      "\n",
      "Bad Answer Tensor\n",
      "Question: Can you suggest a laptop with a Core i7 processor and 16GB of RAM?\n",
      "Context: \n",
      "Available laptops:\n",
      "1. Dell Inspiron: Core i5, 8GB RAM, $800.\n",
      "2. HP Spectre x360: Core i7, 16GB RAM, $1500.\n",
      "3. Apple MacBook Pro: M1 chip, 16GB RAM, $2000.\n",
      "4. Lenovo ThinkPad: AMD Ryzen 5, 8GB RAM, $1000.\n",
      "\n",
      "Answer: Dell Inspiron\n",
      "Tokenized Question: ['can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?']\n",
      "Tokenized Context: ['available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.']\n",
      "Tokenized Answer: ['dell', 'ins', '##pi', '##ron']\n",
      "Combined Tokens (with special tokens): ['[CLS]', 'can', 'you', 'suggest', 'a', 'laptop', 'with', 'a', 'core', 'i', '##7', 'processor', 'and', '16', '##gb', 'of', 'ram', '?', '[SEP]', 'available', 'laptop', '##s', ':', '1', '.', 'dell', 'ins', '##pi', '##ron', ':', 'core', 'i', '##5', ',', '8', '##gb', 'ram', ',', '$', '800', '.', '2', '.', 'hp', 'spec', '##tre', 'x', '##36', '##0', ':', 'core', 'i', '##7', ',', '16', '##gb', 'ram', ',', '$', '1500', '.', '3', '.', 'apple', 'mac', '##book', 'pro', ':', 'm1', 'chip', ',', '16', '##gb', 'ram', ',', '$', '2000', '.', '4', '.', 'len', '##ovo', 'think', '##pad', ':', 'am', '##d', 'ry', '##zen', '5', ',', '8', '##gb', 'ram', ',', '$', '1000', '.', '[SEP]']\n",
      "Start: 25\n",
      "End: 28\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "span = get_answer_span_tensor(test_question,test_context,test_answer)\n",
    "# print(span)\n",
    "assert span.shape == (2,)\n",
    "# uncomment to double check\n",
    "# assert list(span) == [43,48]\n",
    "\n",
    "print(\"\\nBad Answer Tensor\")\n",
    "span = get_answer_span_tensor(test_question,test_context,bad_answer)\n",
    "# uncomment to double check, if the code runs and the assertion error appears then success shouldn't pop up\n",
    "assert list(span) == [25,28]\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_to_BERT_tensors function tokenizes input questions and contexts into BERT's tensor format, including input_ids and attention_mask. These inputs are used for token-level slot labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_BERT_tensors(questions, contexts):\n",
    "    '''takes a parallel list of question strings and answer strings'''\n",
    "    #your code here\n",
    "    result = tokenizer(questions,contexts,  return_tensors=\"pt\", padding='max_length', truncation=True,  max_length=512)\n",
    "    return result['input_ids'], result['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\"Why?\", \"How?\"]\n",
    "test_contexts = [\"I think it is because we can bluminate\", \"It was done\"\" \".join([\"very\"]*1000) + \" well\"]\n",
    "\n",
    "ids, mask = convert_to_BERT_tensors(test_questions,test_contexts)\n",
    "assert ids.shape == (2,512) # 512 because that's the max allowed sequence length for BERT\n",
    "assert ids[0][3] == 102 # fourth token is separator\n",
    "assert list(ids[0][-100:]) == [0]*100 # first row is mostly padding\n",
    "assert list(ids[1][-100:]) != [0]*100 # second row is not\n",
    "assert list(mask[0][-100:]) == [0]*100 # first row padding is masked\n",
    "assert list(mask[1][-100:]) != [0]*100 # second row is not padding, no mask\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, mask = convert_to_BERT_tensors(test_questions, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided code\n",
    "batch_size = 16\n",
    "\n",
    "class QAdataset(Dataset):\n",
    "    '''A dataset for housing QA data, including input_data, output_data, and padding mask'''\n",
    "    def __init__(self, input_data, output_data,mask):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = self.output_data[index]\n",
    "        data_val = self.input_data[index]\n",
    "        mask = self.mask[index]\n",
    "        return data_val,target,mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports and downloads in the two cells below were intended to use for intent classification prediction but because we only have one intent to work with, we don't have to do the extra work with training a classifier to predict the intent. The intent is already fixed, we can focus entirely on slot extraction (e.g., extracting price, brand, RAM, etc.) from the user utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user utterances for training, development, and test\n",
    "with open(\"data/train_utt.txt\") as f:\n",
    "    train = f.read().splitlines()\n",
    "\n",
    "with open(\"data/dev_utt.txt\") as f:\n",
    "    dev = f.read().splitlines()\n",
    "\n",
    "with open(\"data/test_utt.txt\") as f:\n",
    "    test = f.read().splitlines()\n",
    "\n",
    "# Initialize empty lists for category and facility (slots)\n",
    "train_category = []\n",
    "train_facility = []\n",
    "\n",
    "# Load answers (slots and categories) for training data\n",
    "with open(\"data/train_ans.txt\") as f:\n",
    "    for line in f:\n",
    "        splitting = line.strip().split('|')\n",
    "        # The intent (e.g., find_laptop)\n",
    "        train_category.append(splitting[0])  \n",
    "        train_facility.append(splitting[1:])  # Slot-value pairs (e.g., brand=Apple, price=cheap)\n",
    "\n",
    "# Load answers for development data\n",
    "dev_category = []\n",
    "dev_facility = []\n",
    "with open(\"data/dev_ans.txt\") as f:\n",
    "    for line in f:\n",
    "        splitting = line.strip().split('|')\n",
    "        dev_category.append(splitting[0])  \n",
    "        dev_facility.append(splitting[1:])  # Slot-value pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_display_size(utterance):\n",
    "    display_sizes = [\"13-inch\", \"14-inch\", \"15-inch\", \"15.6-inch\", \"17-inch\"]\n",
    "    utterance = utterance.lower()\n",
    "\n",
    "    for size in display_sizes:\n",
    "        if size in utterance:\n",
    "            return f\"display_size={size}\"\n",
    "    return \"display_size=unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(utterance, establishment, filter=[]):\n",
    "    utterance = utterance.lower()\n",
    "    for f in filter:\n",
    "        utterance = utterance.replace(f, \"XXXX\")\n",
    "    \n",
    "    cheap_keywords = [\"cheap\", \"inexpensive\", \"affordable\", \"budget-friendly\", \"low price\"]\n",
    "    expensive_keywords = [\"expensive\", \"high-end\", \"costly\", \"premium\", \"upscale\"]\n",
    "    midrange_keywords = [\"moderate\", \"mid-range\", \"middle price\", \"mid price\"]\n",
    "    dontcare_keywords = [\"price doesn't matter\", \"any price\", \"doesn't care about cost\"]\n",
    "\n",
    "    # Check if the utterance indicates a price range\n",
    "    if any(keyword in utterance for keyword in cheap_keywords):\n",
    "        return f\"{establishment}-price=cheap\"\n",
    "    if any(keyword in utterance for keyword in expensive_keywords):\n",
    "        return f\"{establishment}-price=expensive\"\n",
    "    if any(keyword in utterance for keyword in midrange_keywords):\n",
    "        return f\"{establishment}-price=mid-range\"\n",
    "    if any(keyword in utterance for keyword in dontcare_keywords):\n",
    "        return f\"{establishment}-price=dontcare\"\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ram(utterance, establishment, filter=[]):\n",
    "    utterance = utterance.lower()\n",
    "    for f in filter:\n",
    "        utterance = utterance.replace(f, \"XXXX\")\n",
    "    \n",
    "    ram_sizes = [\"8gb\", \"16gb\", \"32gb\", \"64gb\"]\n",
    "    dontcare_keywords = [\"don't care about memory\", \"any memory size\", \"no preference for RAM\"]\n",
    "\n",
    "    # Check if the utterance specifies a RAM size\n",
    "    for ram in ram_sizes:\n",
    "        if ram in utterance:\n",
    "            return f\"{establishment}-ram_memory={ram.upper()}\"\n",
    "    \n",
    "    # Check if the user doesn't care about RAM\n",
    "    if any(keyword in utterance for keyword in dontcare_keywords):\n",
    "        return f\"{establishment}-ram_memory=dontcare\"\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_brand(utterance, establishment, filter=[]):\n",
    "    utterance = utterance.lower()\n",
    "    for f in filter:\n",
    "        utterance = utterance.replace(f, \"XXXX\")\n",
    "    \n",
    "    brands = [\"apple\", \"dell\", \"hp\", \"samsung\", \"lenovo\", \"asus\", \"microsoft\"]\n",
    "\n",
    "    for brand in brands:\n",
    "        if brand in utterance:\n",
    "            return f\"{establishment}-brand={brand.capitalize()}\"\n",
    "    \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_processor_tier(utterance):\n",
    "    tiers = {\n",
    "        \"high\": [\"high-end\", \"powerful\", \"m1\", \"i9\", \"ryzen 9\"],\n",
    "        \"mid\": [\"mid-range\", \"balanced\", \"i7\", \"ryzen 7\"],\n",
    "        \"low\": [\"low-end\", \"budget\", \"i5\", \"ryzen 5\"]\n",
    "    }\n",
    "    utterance = utterance.lower()\n",
    "\n",
    "    for tier, keywords in tiers.items():\n",
    "        if any(keyword in utterance for keyword in keywords):\n",
    "            return f\"processor_tier={tier}\"\n",
    "    return \"processor_tier=unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the utterance through all slot-prediction functions and combine the results\n",
    "def extract_slots(utterance):\n",
    "    slots = []\n",
    "    slots.append(predict_price(utterance))\n",
    "    slots.append(predict_display_size(utterance))\n",
    "    slots.append(predict_processor_tier(utterance))\n",
    "    slots.append(predict_brand(utterance))\n",
    "    return [slot for slot in slots if \"unknown\" not in slot]  # Filter out unknown slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance: I need a budget-friendly laptop. What’s the cheapest one you have?\n",
      "Expected: ['price=cheap']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: What’s the most affordable laptop with a 15.6-inch display?\n",
      "Expected: ['display_size=15.6', 'price=cheap']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: Can you find me a Dell laptop that’s cheap?\n",
      "Expected: ['brand=Dell', 'price=500']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: Can you find a laptop with a 15.6-inch screen and under $700, but with a high-end processor?\n",
      "Expected: ['display_size=15.6', 'price=700', 'processor_tier=high']\n",
      "Prediction: laptop-price=expensive\n",
      "Utterance: Can you help me find a good and cheap laptop that is suitable for machine learning?\n",
      "Expected: ['ram_memory=16GB', 'processor_tier=m1', 'price=cheap']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: I’m a college student and I’m looking for an affordable laptop\n",
      "Expected: ['brand=lenovo', 'price=cheap']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: Can you help me find a cheap HP laptop for school?\n",
      "Expected: ['brand=HP', 'price=cheap', 'ram_memory=8GB']\n",
      "Prediction: laptop-price=cheap\n",
      "Utterance: Looking for a budget-friendly laptop with a large screen for watching movies.\n",
      "Expected: ['price=cheap', 'display_size=15.6']\n",
      "Prediction: laptop-price=cheap\n",
      "Correct: 32\n",
      "Incorrect: 8\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for utt, ans, cat in list(zip(train, train_facility, train_category)):\n",
    "    # Example: Predict price\n",
    "    pred_price = predict_price(utt, \"laptop\")\n",
    "    if (\"-price\" in \" \".join(ans) and pred_price in \" \".join(ans)) or (\"-price\" not in \" \".join(ans) and pred_price == \"\"):\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(f\"Utterance: {utt}\")\n",
    "        print(f\"Expected: {ans}\")\n",
    "        print(f\"Prediction: {pred_price}\")\n",
    "        incorrect += 1\n",
    "\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Incorrect: {incorrect}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
